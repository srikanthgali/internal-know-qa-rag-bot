# Internal Knowledge Base RAG Chatbot

A production-ready RAG (Retrieval-Augmented Generation) chatbot for internal knowledge base queries using OpenAI, FAISS, and Streamlit.

## ğŸŒŸ Features

- **Document Ingestion**: Support for PDF, DOCX, TXT, MD files
- **Vector Search**: Fast similarity search with FAISS
- **OpenAI Integration**: GPT-4.1 mini for answers, Ada-002 for embeddings
- **REST API**: FastAPI backend with async support
- **Modern UI**: Streamlit chat interface
- **Source Citations**: Automatic source tracking and display
- **Conversational AI**: Handles greetings, introductions, and natural conversations
- **Edge Case Handling**: Smart detection of out-of-scope queries
- **High Performance**: 89.1% overall accuracy with excellent retrieval and relevance scores

## ğŸ“Š Performance Metrics

The RAG system has been thoroughly evaluated with the following results:

| Metric | Score | Description |
|--------|-------|-------------|
| **Retrieval Score** | 90.4% | Accuracy in finding relevant documents |
| **Faithfulness** | 88.9% | Factual accuracy of generated answers |
| **Relevance** | 89.0% | Answer relevance to the question |
| **Completeness** | 87.2% | Coverage of question aspects |
| **Overall Score** | 89.1% | Weighted average performance |

### Evaluation Details

**Test Coverage:**
- âœ… **14 total queries** evaluated across multiple categories
- âœ… **8 factual/procedural queries** - Core knowledge base questions
- âœ… **4 greeting/intro queries** - Conversational handling (100% success rate)
- âœ… **2 edge case queries** - Out-of-scope detection (100% success rate)

**Query Categories Tested:**
- **Conversational** (Greetings, introductions) - Perfect handling
- **Factual** (GitLab's mission, purpose, customer acceptance)
- **Technical** (Training, knowledge sharing, remote work)
- **Procedural** (Time off requests, contribution processes)
- **Edge Cases** (Weather, sports) - Correctly rejected

**Key Strengths:**
- ğŸ¯ **Perfect edge case handling** (100%) - Correctly identifies out-of-scope questions
- ğŸ¯ **Perfect greeting handling** (100%) - Natural conversational responses
- ğŸ¯ **Excellent retrieval** (90.4%) - Highly accurate document matching
- ğŸ¯ **Strong relevance** (89.0%) - Answers well-aligned with questions
- ğŸ¯ **High faithfulness** (88.9%) - Factual accuracy with minimal hallucinations
- ğŸ¯ **Good completeness** (87.2%) - Comprehensive answer coverage

**Quality Consistency:**
- Faithfulness standard deviation: 5.0% (consistent quality)
- Completeness standard deviation: 9.2% (reliable coverage)
- Relevance standard deviation: 8.0% (stable relevance)

**Continuous Improvement:**
- System demonstrates robust performance across diverse query types
- Edge case detection prevents hallucinations on out-of-scope questions
- Conversational abilities enhance user experience
- Ongoing monitoring and refinement of retrieval and generation quality

*Last evaluation: December 29, 2025*
*Evaluation framework: 14 test cases across 5 categories*

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone <your-repo-url>
cd internal-know-qa-rag-bot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=your_api_key_here
```

### 3. Ingest Documents

```bash
# Option 1: Download from website
python main.py ingest --url https://handbook.gitlab.com/handbook/engineering --max-pages 50

# Option 2: Place documents in data/raw/ manually
# Then skip to building the index
```

### 4. Build Vector Index

```bash
python main.py build-index --data-dir data/raw
```

### 5. Run Application

```bash
# Terminal 1: Start API server
python main.py api

# Terminal 2: Start Streamlit UI
python main.py ui
```

Visit http://localhost:8501 to use the chatbot!

## ğŸ“‹ Architecture

```
ğŸ“„ Documents â†’ ğŸ“¦ Chunks â†’ ğŸ”¢ Embeddings â†’ ğŸ’¾ FAISS Index
                                                    â†“
â“ User Question â†’ ğŸ”¢ Query Embedding â†’ ğŸ” Similarity Search
                                                    â†“
ğŸ“‹ Top-K Chunks â†’ ğŸ“ Prompt + Context â†’ ğŸ¤– LLM â†’ ğŸ’¬ Answer
```

## ğŸ› ï¸ Technology Stack

- **LLM**: OpenAI GPT-4.1 mini
- **Embeddings**: OpenAI Ada-002
- **Vector Store**: FAISS
- **API**: FastAPI
- **UI**: Streamlit
- **Document Processing**: PyPDF2, python-docx, LangChain

## ğŸ“‚ Project Structure

```
internal-know-qa-rag-bot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/           # Configuration management
â”‚   â”œâ”€â”€ ingestion/        # Document loading
â”‚   â”œâ”€â”€ embeddings/       # Embedding generation & indexing
â”‚   â”œâ”€â”€ retrieval/        # Document retrieval & RAG pipeline
â”‚   â”œâ”€â”€ generation/       # LLM integration
â”‚   â””â”€â”€ utils/            # Helper functions
â”œâ”€â”€ api/                  # FastAPI endpoints
â”œâ”€â”€ ui/                   # Streamlit interface
â”‚   â””â”€â”€ streamlit_app/
â”‚       â”œâ”€â”€ app.py        # Main UI application
â”‚       â”œâ”€â”€ components/   # UI components (chat, sidebar)
â”‚       â””â”€â”€ styles/       # Custom CSS styling
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Raw documents
â”‚   â””â”€â”€ processed/       # Processed chunks
â”œâ”€â”€ vector_store/        # FAISS index
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â””â”€â”€ demo_evaluation.py  # Evaluation script
â”œâ”€â”€ tests/               # Test suite & evaluation
â”‚   â”œâ”€â”€ test_questions.json  # Evaluation test cases
â”‚   â””â”€â”€ test_*.py        # Unit tests
â”œâ”€â”€ evaluation_report.json   # Latest evaluation results
â””â”€â”€ config.yaml          # Application configuration
â””â”€â”€ assets/              # Static assets
    â””â”€â”€ demo.gif         # Demo gif video
```

## ğŸ”§ Configuration

Edit `config.yaml` to customize:

- Model settings (GPT-4, temperature, max tokens)
- Chunk size and overlap
- Retrieval parameters (top-k, threshold)
- API and UI ports

## ğŸ“– Usage Examples

### Conversational Queries

```bash
# Greeting
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Hi!", "max_sources": 5}'

# Introduction request
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What can you do?", "max_sources": 5}'
```

### Knowledge Base Queries

```bash
# Ask a factual question
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is GitLab'\''s mission?", "max_sources": 5}'

# Ask a procedural question
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How do I request time off?", "max_sources": 5}'
```

### API Health Check

```bash
# Health check
curl http://localhost:8000/health
```

### Python

```python
from src.retrieval.rag_pipeline import RAGPipeline

# Initialize pipeline
pipeline = RAGPipeline()

# Ask question
result = pipeline.query("What is our code review process?")

print(result["answer"])
print(result["sources"])

# Handle greetings
greeting_result = pipeline.query("Hello!")
print(greeting_result["answer"])  # Returns friendly greeting
print(greeting_result.get("is_greeting"))  # True
```
## ğŸ“º Demo

Here's a quick demo of the GitLab KnowledgeBase QA chatbot in action:
> **App Demo:**
>
> [![Watch the demo video](assets/demo.gif)]

## ğŸ§ª Testing & Evaluation

```bash
# Run comprehensive evaluation
python scripts/demo_evaluation.py

# Run unit tests
pytest tests/

# Run specific test
pytest tests/test_retrieval.py

# View latest evaluation results
cat evaluation_report.json
```

### Evaluation Framework

The system uses a comprehensive evaluation framework that measures:

1. **Retrieval Quality**: Relevance of retrieved documents (90.4%)
2. **Faithfulness**: Factual accuracy without hallucinations (88.9%)
3. **Relevance**: Answer alignment with the question (89.0%)
4. **Completeness**: Coverage of all question aspects (87.2%)
5. **Edge Case Handling**: Out-of-scope query detection (100%)
6. **Conversational Handling**: Greeting and introduction responses (100%)

**Test Categories:**
- âœ… **Conversational**: Greetings, introductions, help requests
- âœ… **Factual**: Company mission, purpose, values, policies
- âœ… **Technical**: Training, knowledge sharing, platform features
- âœ… **Procedural**: Time off requests, contribution processes
- âœ… **Policy**: Customer acceptance, remote work guidelines
- âœ… **Edge Cases**: Out-of-scope queries (weather, sports, etc.)

**Special Features:**
- **Greeting Detection**: Automatically recognizes conversational openings
- **Introduction Requests**: Explains capabilities and usage
- **Out-of-Scope Detection**: Politely declines irrelevant questions
- **Source Attribution**: Tracks and displays relevant sources (when applicable)

## ğŸ¨ UI Features

- **Fixed Header**: GitLab-branded orange gradient header stays visible while scrolling
- **Collapsible Sidebar**: Settings and controls in a collapsible side panel
- **Chat Interface**: Streamlit's native chat UI with source expandables
- **Responsive Design**: Works on desktop and mobile devices
- **Custom Styling**: GitLab color scheme

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ‘¤ Author

Srikanth Gali (srikanthgali137@gmail.com)
